{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890f0a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/jcai2/miniconda3/envs/fastvideo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "/u/jcai2/miniconda3/envs/fastvideo/lib/python3.11/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.10 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import AutoencoderKL, DDPMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from einops import rearrange\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# 假设我们有DiT模型定义，类似于diffusers中的UNet\n",
    "# from my_models import DiTModel, DiTConfig # 这是一个假设的模型定义\n",
    "from diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel, AutoencoderKLHunyuanVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760c4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FlowMatchEulerDiscreteScheduler\n",
    "import json\n",
    "\n",
    "# 读取 scheduler_config.json\n",
    "scheduler_config_path = \"scheduler_config.json\"\n",
    "with open(scheduler_config_path, \"r\") as f:\n",
    "    scheduler_config = json.load(f)\n",
    "\n",
    "# 用配置初始化 scheduler\n",
    "scheduler = FlowMatchEulerDiscreteScheduler.from_config(scheduler_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f325a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel, AutoencoderKLHunyuanVideo\n",
    "vae = AutoencoderKLHunyuanVideo.from_config(\"HunyuanConfig/vae.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10edb93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fm_from_pred_velocity_to_pred_video(model_output, noisy_latents, timestep):\n",
    "    \"\"\"\n",
    "    from velocity to video\n",
    "    \"\"\"\n",
    "    sigmas = scheduler.sigmas\n",
    "    schedule_timesteps = scheduler.timesteps\n",
    "    step_indices = [scheduler.index_for_timestep(t, schedule_timesteps) for t in timestep]\n",
    "    sigma = sigmas[step_indices].flatten()\n",
    "    \n",
    "    # 调整 sigma 的维度用于广播\n",
    "    while len(sigma.shape) < len(noisy_latents.shape):\n",
    "        sigma = sigma.unsqueeze(-1)\n",
    "        \n",
    "    # x_clean_pred = x_t + t * v_pred\n",
    "    pred_video = noisy_latents + sigma * model_output\n",
    "    \n",
    "    return pred_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b04dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scheduler.sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "792d11d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([846.6813])\n"
     ]
    }
   ],
   "source": [
    "model_output = torch.randn((1, 3, 16, 224, 224))\n",
    "noisy_latents = torch.randn((1, 3, 16, 224, 224))\n",
    "\n",
    "def _sample_timestep() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    随机选择一个时间步，用于生成噪声，返回形状为 [batch_size] 的 tensor。\n",
    "    \"\"\"\n",
    "    idx = torch.randint(0, len(scheduler.timesteps), (1,))\n",
    "    sampled_timesteps = scheduler.timesteps[idx]\n",
    "    return sampled_timesteps\n",
    "timestep = _sample_timestep()\n",
    "print(timestep)\n",
    "x = _fm_from_pred_velocity_to_pred_video(model_output, noisy_latents, list(timestep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefdfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/jcai2/miniconda3/envs/fastvideo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel, AutoencoderKLHunyuanVideo\n",
    "from transformers import LlamaModel, CLIPTextModel\n",
    "import torch\n",
    "model_pipe = HunyuanVideoPipeline.from_pretrained(\n",
    "            \"hunyuanvideo-community/HunyuanVideo\", \n",
    "            transformer=None,\n",
    "            vae=None,\n",
    "            text_encoder_2=None,\n",
    "            tokenizer_2=None,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "text = \"<PAD>\"\n",
    "num_hidden_layers_to_skip = 2\n",
    "text_inputs = model_pipe.tokenizer(\n",
    "    text, padding=\"max_length\", max_length=10, return_tensors=\"pt\"\n",
    ")\n",
    "text_input_ids = text_inputs.input_ids.to(\"cuda\")\n",
    "prompt_attention_mask = text_inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "prompt_embeds = model_pipe.text_encoder(\n",
    "            input_ids=text_input_ids,\n",
    "            attention_mask=prompt_attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        ).hidden_states[-(num_hidden_layers_to_skip + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c258c4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'vae_latent_bytes', 'vae_latent_shape', 'vae_latent_dtype', 'text_embedding_bytes', 'text_embedding_shape', 'text_embedding_dtype', 'pooled_text_embedding_bytes', 'pooled_text_embedding_shape', 'pooled_text_embedding_dtype', 'text_attention_mask_bytes', 'text_attention_mask_shape', 'text_attention_mask_dtype', 'file_name', 'caption', 'media_type', 'width', 'height', 'num_frames', 'duration_sec', 'fps'])\n",
      "mixkit-industrial-machine-working-2230_clip_1\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 读取整个 parquet\n",
    "table = pq.read_table(\"/work/hdd/bcjw/jcai2/dataset/mixkit-processed/combined_parquet_dataset/worker_0/data_chunk_0.parquet\")\n",
    "\n",
    "# 转成字典或 DataFrame\n",
    "data = table.to_pydict()  \n",
    "print(data.keys())  # 看有哪些字段\n",
    "print(data[\"id\"][0])  # 取第一条的 id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99faff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixkit-industrial-machine-working-2230_clip_1\n",
      "torch.Size([16, 21, 60, 104]) torch.float32\n",
      "The video captures an intricate close-up of a high-precision CNC machining tool operating with mechanical finesse. The tool head, prominently centered, rotates smoothly and rhythmically as it carefully hones a metal workpiece, demonstrating precise industrial craftsmanship. The metallic surface of the tool gleams under soft, diffuse lighting, highlighting its polished finish and utilitarian design. Shadows play across the surrounding machinery, adding depth and contrast to the industrial scene. Subtle reflections are visible on the pristine metal surface, emphasizing its high-quality engineering and maintenance. The cool, muted tones of the machining environment suggest a place of focused technical activity. This scene embodies a symphony of engineering precision and mechanical elegance, ideal for recreations geared towards industrial realism.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_883583/129812600.py:16: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  return torch.from_numpy(np_array)  # 返回 torch.Tensor\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_parquet_record(path, idx=0):\n",
    "    \"\"\"读取 parquet 文件中的第 idx 条记录，并还原所有张量\"\"\"\n",
    "    table = pq.read_table(path)\n",
    "    data = table.to_pydict()  # 转成 Python dict（列名 -> list）\n",
    "\n",
    "    def restore_tensor(prefix):\n",
    "        \"\"\"根据字段前缀还原 numpy/tensor\"\"\"\n",
    "        arr_bytes = data[f\"{prefix}_bytes\"][idx]\n",
    "        arr_shape = data[f\"{prefix}_shape\"][idx]\n",
    "        arr_dtype = data[f\"{prefix}_dtype\"][idx]\n",
    "        np_array = np.frombuffer(arr_bytes, dtype=arr_dtype).reshape(arr_shape)\n",
    "        return torch.from_numpy(np_array)  # 返回 torch.Tensor\n",
    "\n",
    "    record = {\n",
    "        \"id\": data[\"id\"][idx],\n",
    "        \"caption\": data[\"caption\"][idx],\n",
    "        \"file_name\": data[\"file_name\"][idx],\n",
    "        \"media_type\": data[\"media_type\"][idx],\n",
    "        \"width\": data[\"width\"][idx],\n",
    "        \"height\": data[\"height\"][idx],\n",
    "        \"num_frames\": data[\"num_frames\"][idx],\n",
    "        \"duration_sec\": data[\"duration_sec\"][idx],\n",
    "        \"fps\": data[\"fps\"][idx],\n",
    "        \"vae_latent\": restore_tensor(\"vae_latent\"),\n",
    "        \"text_embedding\": restore_tensor(\"text_embedding\"),\n",
    "        \"pooled_text_embedding\": restore_tensor(\"pooled_text_embedding\"),\n",
    "        \"text_attention_mask\": restore_tensor(\"text_attention_mask\"),\n",
    "    }\n",
    "    return record\n",
    "\n",
    "# 例子：读取第一条\n",
    "record = load_parquet_record(\"/work/hdd/bcjw/jcai2/dataset/mixkit-processed/combined_parquet_dataset/worker_0/data_chunk_0.parquet\", idx=0)\n",
    "print(record[\"id\"])\n",
    "print(record[\"vae_latent\"].shape, record[\"vae_latent\"].dtype)\n",
    "print(record[\"caption\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastvideo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
